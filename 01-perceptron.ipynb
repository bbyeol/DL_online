{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset \n",
    "y = 0.3(w) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.34116805,  0.88030514, -0.24372275, -0.13107826,  0.72047141,\n",
       "       -0.46770595, -0.81556937, -0.11333545,  0.30322765, -0.61047795])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.random.uniform(low = -1.0, high = 1.0, size = 100)  ## uniform distribution \n",
    "\n",
    "\n",
    "print(x1.shape)\n",
    "x1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.15023147,  0.45373934, -0.69773579, -0.77133318, -0.59449272,\n",
       "        0.16721176, -0.46444847, -0.37763371, -0.00135709,  0.15476108])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.random.uniform(low = -1.0, high = 1.0, size = 100)  ## uniform distribution \n",
    "\n",
    "\n",
    "print(x2.shape)\n",
    "x2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.07746615,  0.59096121, -0.32198472, -0.32499007,  0.01889506,\n",
       "        0.04329409, -0.37689505, -0.12281749,  0.19028975, -0.00576284])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0.3 * x1 + 0.5 * x2 + 0.1 ## add bias, x3 is always 1\n",
    "#y = 0.3 * x1 + 0.5 * x2 + 0.1 * x3 ## add bias, x3 is always 1\n",
    "\n",
    "print(y.shape)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search(2-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.04085767418353488\n",
      "1 -0.07761777274638039\n",
      "2 0.06246498160686528\n",
      "3 0.01040624498481292\n",
      "4 -0.026295832742901634\n",
      "5 -0.007124588336770392\n",
      "6 0.0028326537732120804\n",
      "7 -0.01629576707258473\n",
      "8 0.058462311990106805\n",
      "9 -0.10041360545149608\n",
      "10 -0.0017797011417267667\n",
      "11 -0.04546422808685691\n",
      "12 0.023202095695320227\n",
      "13 -0.10846193063133153\n",
      "14 0.021982241124565816\n",
      "15 -0.06630189453461263\n",
      "16 0.03190508105381825\n",
      "17 0.059349811427822356\n",
      "18 -0.011686530662953583\n",
      "19 -0.11953305686582133\n",
      "20 0.02044807124648227\n",
      "21 -0.018250278293089922\n",
      "22 -0.004107342333873546\n",
      "23 0.04178828550838737\n",
      "24 -0.06273512941667475\n",
      "25 -0.08759130165509894\n",
      "26 -0.04883999087278013\n",
      "27 -0.06575972354607316\n",
      "28 -0.020316003534003445\n",
      "29 0.03416090845141733\n",
      "30 -0.11911198727192725\n",
      "31 -0.08460424392643823\n",
      "32 0.023962559842600473\n",
      "33 -0.06347136846928238\n",
      "34 0.06231637216009244\n",
      "35 -0.06658127258830823\n",
      "36 -0.0211779517363936\n",
      "37 0.014282527908827065\n",
      "38 0.019151954850609883\n",
      "39 -0.08631588696252965\n",
      "40 0.0566416322281513\n",
      "41 -0.04305541907720941\n",
      "42 -0.02951190744760891\n",
      "43 -0.08520269113786962\n",
      "44 0.01718240837323772\n",
      "45 0.030768664481914385\n",
      "46 -0.012917186642251282\n",
      "47 -0.01861464871876887\n",
      "48 -0.018723267629163796\n",
      "49 0.03169752823965487\n",
      "50 0.02538946606824499\n",
      "51 -0.027463658526402864\n",
      "52 -0.0002917765707560506\n",
      "53 -0.007591549043918653\n",
      "54 -0.04726497783988158\n",
      "55 0.037397076943560995\n",
      "56 -0.060691816233793806\n",
      "57 0.025297094626211835\n",
      "58 -0.10084257379921419\n",
      "59 0.036054754224859105\n",
      "60 -0.006649473636685321\n",
      "61 -0.1196292146259802\n",
      "62 -0.08327801674657417\n",
      "63 0.034361769077287045\n",
      "64 0.003100425476695856\n",
      "65 -0.019474807107607007\n",
      "66 -0.09841158187060489\n",
      "67 0.058264135731941884\n",
      "68 -0.012268769484532989\n",
      "69 -0.03709387554669843\n",
      "70 0.05146246526596937\n",
      "71 -0.11525867061326701\n",
      "72 0.03804565655749719\n",
      "73 0.013274908090933603\n",
      "74 -0.04851350179374683\n",
      "75 -0.06633494512988108\n",
      "76 -0.04600852152964219\n",
      "77 0.0013138665702668534\n",
      "78 -0.06481051985488509\n",
      "79 -0.048892584160345705\n",
      "80 -0.07719482992640975\n",
      "81 -0.10953594567000406\n",
      "82 -0.10165093540789515\n",
      "83 0.030847626172316418\n",
      "84 0.061775469585253776\n",
      "85 -0.05745308359738892\n",
      "86 0.06361540700798232\n",
      "87 0.012418265207417201\n",
      "88 0.04616335563304629\n",
      "89 -0.09461339757799178\n",
      "90 0.0648325728241421\n",
      "91 -0.09580282847362252\n",
      "92 -0.04249079097488623\n",
      "93 0.0385832442812762\n",
      "94 0.04993402852151795\n",
      "95 -0.05756973024530649\n",
      "96 -0.0881568500749954\n",
      "97 -0.03319404671652624\n",
      "98 0.058006043568431004\n",
      "99 -0.07091615648729159\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "for epoch in range(num_epoch):\n",
    "    w1 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "    #w1 = 0.3  # for test \n",
    "    y_predict = w1 *x1 ## we expect w1 is 0.3\n",
    "    \n",
    "    error = (y_predict-y).mean() # error = 0 is the  best model\n",
    "    \n",
    "    print(epoch, error)\n",
    "    #break # print result of first loop for easy debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## exception\n",
    "y_example = [-1, 0, +1]\n",
    "y_predict_example = [+1, -1, 0]\n",
    "\n",
    "y_example = np.array(y_example)\n",
    "y_predict_example = np.array(y_predict_example)\n",
    "\n",
    "error = (y_predict_example-y_example).mean()\n",
    "error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3333333333333333"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Actually y_example and y_predicte_example are nor same, bur error is zero.\n",
    "## To solve this problem, change (y_predict_example-y_example) plus values\n",
    "\n",
    "np.abs(y_predict_example-y_example).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.9162682413581955 0.6496841114503242\n",
      "1 -0.07208862437559471 0.19875555332948824\n",
      "2 0.6226048463484934 0.17232320619951463\n",
      "3 0.9851221808560129 0.36596614148807627\n",
      "4 0.1660322105153178 0.07156048420464117\n",
      "5 0.22437249982617113 0.04039732649499733\n",
      "6 -0.6593010545733309 0.5124220398601085\n",
      "7 -0.30141018542864373 0.32125038593547617\n",
      "8 0.859726338871881 0.2989844647421211\n",
      "9 -0.06578399137123592 0.19538785880933596\n",
      "10 0.1584257103991109 0.07562358648849771\n",
      "11 -0.9387249529693102 0.6616796303934737\n",
      "12 0.541655405226511 0.1290831018052677\n",
      "13 0.05519913114135999 0.13076328852350103\n",
      "14 0.13356256661188182 0.0889045297294636\n",
      "15 0.47641406827714783 0.0942336676225204\n",
      "16 -0.6645177770706543 0.5152086140754905\n",
      "17 0.11188229189916377 0.10048530569135351\n",
      "18 -0.9670438290925507 0.6768064940620127\n",
      "19 -0.3652255102509412 0.35533809882173095\n",
      "20 0.6162235378383356 0.16891455454822865\n",
      "21 -0.5950812763720545 0.47811828340286366\n",
      "22 0.7436572380833157 0.2369847774622416\n",
      "23 0.21279317314921542 0.04658256122159863\n",
      "24 0.6766239407642192 0.20117814638741371\n",
      "25 -0.7101214901784625 0.5395683784940669\n",
      "26 0.5175046289235496 0.11618267810785725\n",
      "27 0.3356247563254253 0.019029386258637034\n",
      "28 -0.7302170377019139 0.5503026536259259\n",
      "29 -0.3309587889580552 0.33703412308803565\n",
      "30 -0.6666129151859892 0.516327756957415\n",
      "31 0.8800883227831116 0.3098610600316283\n",
      "32 0.5675492187343416 0.14291458950580116\n",
      "33 0.16645902750416952 0.07133249484610593\n",
      "34 -0.858003257838295 0.6185611792223781\n",
      "35 0.6458612414595424 0.1847458855719424\n",
      "36 0.5305413656808768 0.12314640572022816\n",
      "37 -0.9318890289446544 0.6580281404713648\n",
      "38 0.016065459740437937 0.1516670034009903\n",
      "39 -0.22646676465090687 0.28121846856611316\n",
      "40 0.8401406017078299 0.2885225108623448\n",
      "41 0.788163183333167 0.26075815615510095\n",
      "42 -0.17992140848974691 0.25635571433853127\n",
      "43 -0.9504547434560031 0.6679452371492134\n",
      "44 -0.11760531222487547 0.2230688321737172\n",
      "45 0.5519065665234815 0.1345588812362253\n",
      "46 -0.5288116439379227 0.44271957298678793\n",
      "47 0.3799058918611675 0.04268268017546599\n",
      "48 -0.5770517357144445 0.4684875903503866\n",
      "49 0.7180138337238371 0.2232870487792621\n",
      "50 0.9909619875217863 0.36908554350457384\n",
      "51 0.7852236798046417 0.2591879854698067\n",
      "52 -0.5206297550878676 0.4383491199840872\n",
      "53 -0.21648188670531443 0.2758849275465941\n",
      "54 0.8991524054955702 0.32004436599695396\n",
      "55 -0.5894027810414417 0.47508504774990923\n",
      "56 0.8940624478827772 0.3173255047486372\n",
      "57 0.802041908253083 0.26817164173422786\n",
      "58 0.7902008157405278 0.26184658168083885\n",
      "59 -0.007224926545615462 0.16410783956283262\n",
      "60 0.141336810724348 0.0847518249998183\n",
      "61 0.8300705919382994 0.28314349566902874\n",
      "62 0.2757666891410031 0.012944510509742246\n",
      "63 0.12149954600563162 0.0953481353071501\n",
      "64 -0.4040642648891659 0.3760842804114063\n",
      "65 -0.028543547230458266 0.17549543373491427\n",
      "66 -0.8024333648356758 0.5888778615699493\n",
      "67 0.6468584265182395 0.1852785438020774\n",
      "68 -0.4408547058862564 0.39573633096759686\n",
      "69 0.32029104679298337 0.010838703386166921\n",
      "70 0.6438238894950874 0.18365760984810997\n",
      "71 -0.6189455348886563 0.49086566022532013\n",
      "72 -0.1772803660249569 0.25494497017150136\n",
      "73 -0.48849759924412783 0.4211853477104549\n",
      "74 0.7624950347068284 0.24704721002840152\n",
      "75 0.3133862336685407 0.007150415534077097\n",
      "76 -0.4106367632955188 0.37959505841410385\n",
      "77 0.9572978309377951 0.35110343485922135\n",
      "78 -0.39009041491851915 0.368619982657827\n",
      "79 -0.9842333268151491 0.6859884682141231\n",
      "80 -0.39238227722449825 0.3698442081002044\n",
      "81 -0.28128364551745566 0.3104995558520005\n",
      "82 -0.20235181549706938 0.2683371823654759\n",
      "83 -0.6310999587496078 0.49735808993595276\n",
      "84 -0.6287051273380213 0.4960788623241453\n",
      "85 0.2647876021897455 0.018809120065359973\n",
      "86 -0.6236037289424046 0.49335388984594486\n",
      "87 -0.05198842189559483 0.1880187916973604\n",
      "88 -0.8779403108691932 0.6292107926405933\n",
      "89 0.29783961275452997 0.001153996478930051\n",
      "90 0.33037879385521873 0.016227193164824117\n",
      "91 0.8687993476712121 0.30383092003835743\n",
      "92 0.2530508587933118 0.025078440800330032\n",
      "93 0.7365285137251718 0.23317688476809123\n",
      "94 -0.009287916405956498 0.16520981007293162\n",
      "95 -0.6766645410356877 0.5216969521618382\n",
      "96 -0.20123668642361037 0.2677415229405174\n",
      "97 -0.09005261436827539 0.20835123171657202\n",
      "98 0.11161741101600442 0.10062679495775367\n",
      "99 -0.7009626784120764 0.5346760905249749\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w1 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "    #w1 = 0.3  # for test \n",
    "    y_predict = w1 *x1 ## we expect w1 is 0.3\n",
    "    \n",
    "    error = np.abs(y_predict-y).mean() # Mean absolute error\n",
    "    \n",
    "    print(epoch, w1, error)\n",
    "    #break # print result of first loop for easy debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, w1 = 0.795955, w2 = 0.757174, b = 0.562341, error = 0.603236\n",
      "1, w1 = -0.686308, w2 = 0.992658, b = 0.230408, error = 0.545089\n",
      "2, w1 = 0.376908, w2 = 0.280026, b = -0.304498, error = 0.290749\n",
      "10, w1 = 0.087526, w2 = 0.720423, b = -0.198994, error = 0.240668\n",
      "60, w1 = 0.405190, w2 = 0.164893, b = 0.041696, error = 0.180628\n",
      "70, w1 = 0.183775, w2 = 0.327370, b = -0.055928, error = 0.106925\n",
      "449, w1 = 0.115415, w2 = 0.442784, b = 0.070600, error = 0.105995\n",
      "761, w1 = 0.283402, w2 = 0.516449, b = 0.098233, error = 0.096162\n",
      "1044, w1 = 0.268079, w2 = 0.671358, b = -0.043693, error = 0.095217\n",
      "1280, w1 = 0.385548, w2 = 0.592317, b = -0.007134, error = 0.060926\n",
      "1811, w1 = 0.322041, w2 = 0.476169, b = 0.034139, error = 0.036957\n",
      "4145, w1 = 0.333215, w2 = 0.552034, b = -0.001097, error = 0.030184\n",
      "4755, w1 = 0.278214, w2 = 0.492948, b = 0.027506, error = 0.025617\n",
      "72362, w1 = 0.275551, w2 = 0.516270, b = 0.010847, error = 0.015519\n",
      "309826, w1 = 0.301330, w2 = 0.513863, b = 0.011275, error = 0.011206\n",
      "425133, w1 = 0.287889, w2 = 0.508855, b = -0.006203, error = 0.009641\n",
      "--------------------------------------------------\n",
      "425133, w1 = 0.287889, w2 = 0.508855, b = -0.006203, error = 0.009641\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 1000000\n",
    "\n",
    "best_error = 9999\n",
    "best_w1 = None\n",
    "best_w2 = None\n",
    "best_b = None\n",
    "best_epoch = None\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w1 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "    #w1 = 0.3  # for test \n",
    "    w2 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "    b = np.random.uniform(low = -1.0, high = 1.0)\n",
    "    \n",
    "    #y_predict = w1 *x1 ## we expect w1 is 0.3\n",
    "    y_predict = w1 *x1 + w2 *x2 +b\n",
    "    \n",
    "    error = np.abs(y_predict-y).mean() # Mean absolute error\n",
    "    \n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_w1 = w1\n",
    "        best_w2 = w2\n",
    "        best_b = b\n",
    "        best_epoch = epoch\n",
    "        print(f\"{best_epoch}, w1 = {best_w1:.6f}, w2 = {best_w2:.6f}, b = {best_b:.6f}, error = {best_error:.6f}\")\n",
    "print(\"-----\"*10)\n",
    "print(f\"{best_epoch}, w1 = {best_w1:.6f}, w2 = {best_w2:.6f}, b = {best_b:.6f}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h-step search(2-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, w1 = 0.100000, error = 0.106832\n",
      "1, w1 = 0.200000, error = 0.053416\n",
      "2, w1 = 0.300000, error = 0.000000\n",
      "--------------------------------------------------\n",
      "3, w1 = 0.300000, error = 0.000000\n"
     ]
    }
   ],
   "source": [
    "# num_epoch = 100\n",
    "# h = 0.1\n",
    "\n",
    "# #w1 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "# w1 = 0.0\n",
    "\n",
    "# for epoch in range(num_epoch):\n",
    "#     y_predict = w1 * x1\n",
    "    \n",
    "#     current_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "#     y_predict = (w1+h)* x1\n",
    "#     h_plus_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "#     if h_plus_error < current_error:\n",
    "#         w1 = w1+h\n",
    "#         print(f\"{epoch}, w1 = {w1:.6f}, error = {h_plus_error:.6f}\")\n",
    "#         continue\n",
    "    \n",
    "#     break\n",
    "    \n",
    "# print(\"-----\"*10)\n",
    "# print(f\"{epoch}, w1 = {w1:.6f}, error = {current_error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, w1 = 0.328702, error = 0.015332\n",
      "--------------------------------------------------\n",
      "1, w1 = 0.328702, error = 0.015332\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "h = 0.1 ### must set up everytime\n",
    "\n",
    "w1 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "#w1 = 0.0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w1 * x1\n",
    "    \n",
    "    current_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    y_predict = (w1+h)* x1\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if h_plus_error < current_error:\n",
    "        w1 = w1+h\n",
    "        print(f\"{epoch}, w1 = {w1:.6f}, error = {h_plus_error:.6f}\")\n",
    "        continue\n",
    "    \n",
    "        \n",
    "    y_predict = (w1-h)* x1\n",
    "    h_minus_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if h_minus_error < current_error:\n",
    "        w1 = w1-h\n",
    "        print(f\"{epoch}, w1 = {w1:.6f}, error = {h_minus_error:.6f}\")\n",
    "        continue\n",
    "    \n",
    "    break\n",
    "    \n",
    "print(\"-----\"*10)\n",
    "print(f\"{epoch}, w1 = {w1:.6f}, error = {current_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent(not yet)(2-7,8,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, w1 = 0.159102, w2 = -0.456131, b = 0.069370, error = 0.824195\n",
      "1, w1 = 0.200561, w2 = -0.128972, b = 0.085893, error = 0.485645\n",
      "2, w1 = 0.229643, w2 = 0.086338, b = 0.091369, error = 0.319434\n",
      "3, w1 = 0.250462, w2 = 0.227903, b = 0.094800, error = 0.210341\n",
      "4, w1 = 0.265264, w2 = 0.320996, b = 0.096892, error = 0.138541\n",
      "5, w1 = 0.275732, w2 = 0.382222, b = 0.098162, error = 0.091262\n",
      "6, w1 = 0.283099, w2 = 0.422496, b = 0.098926, error = 0.060127\n",
      "7, w1 = 0.288264, w2 = 0.448991, b = 0.099383, error = 0.039619\n",
      "8, w1 = 0.291871, w2 = 0.466424, b = 0.099653, error = 0.026109\n",
      "9, w1 = 0.294383, w2 = 0.477896, b = 0.099810, error = 0.017209\n",
      "10, w1 = 0.296127, w2 = 0.485446, b = 0.099901, error = 0.011345\n",
      "11, w1 = 0.297334, w2 = 0.490416, b = 0.099951, error = 0.007482\n",
      "12, w1 = 0.298169, w2 = 0.493688, b = 0.099979, error = 0.004937\n",
      "13, w1 = 0.298744, w2 = 0.495842, b = 0.099993, error = 0.003258\n",
      "14, w1 = 0.299140, w2 = 0.497261, b = 0.100000, error = 0.002150\n",
      "15, w1 = 0.299412, w2 = 0.498195, b = 0.100003, error = 0.001420\n",
      "16, w1 = 0.299598, w2 = 0.498811, b = 0.100004, error = 0.000938\n",
      "17, w1 = 0.299726, w2 = 0.499216, b = 0.100004, error = 0.000620\n",
      "--------------------------------------------------\n",
      "18, w1 = 0.299726, w2 = 0.499216, b = 0.100004, error = 0.000410\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "w1 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "w2 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "b = np.random.uniform(low = -1.0, high = 1.0)\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch): \n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    error = np.abs(y_predict-y).mean()\n",
    "    \n",
    "    if error < 0.0005:\n",
    "        break\n",
    "    ## update weights differentially\n",
    "    w1 = w1 - ((y_predict - y)*x1).mean()\n",
    "    w2 = w2 - ((y_predict - y)*x2).mean()\n",
    "    b = b - ((y_predict - y)).mean() ## x3 is always 1\n",
    "    \n",
    "    print(f\"{epoch}, w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:.6f}\")\n",
    "    \n",
    "print(\"-----\"*10)\n",
    "print(f\"{epoch}, w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:.6f}\")\n",
    "\n",
    "### more features available with gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent(2-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, w1 = 0.335976, w2 = 0.244401, b = 0.023666, error = 0.276375\n",
      "1, w1 = 0.325011, w2 = 0.356672, b = 0.108800, error = 0.141626\n",
      "2, w1 = 0.310192, w2 = 0.421643, b = 0.088849, error = 0.073548\n",
      "3, w1 = 0.305686, w2 = 0.456508, b = 0.099127, error = 0.040632\n",
      "4, w1 = 0.302507, w2 = 0.476061, b = 0.097917, error = 0.021988\n",
      "5, w1 = 0.301244, w2 = 0.486753, b = 0.099412, error = 0.012206\n",
      "6, w1 = 0.300541, w2 = 0.492689, b = 0.099523, error = 0.006691\n",
      "7, w1 = 0.300239, w2 = 0.495957, b = 0.099801, error = 0.003702\n",
      "8, w1 = 0.300091, w2 = 0.497766, b = 0.099878, error = 0.002040\n",
      "9, w1 = 0.300030, w2 = 0.498764, b = 0.099941, error = 0.001128\n",
      "10, w1 = 0.300004, w2 = 0.499316, b = 0.099967, error = 0.000623\n",
      "--------------------------------------------------\n",
      "11, w1 = 0.300004, w2 = 0.499316, b = 0.099967, error = 0.000345\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "learning_rate = 1.3 ## set up myself to control update speed --> hyperparameter --> deep learning algorithm performance\n",
    "\n",
    "w1 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "w2 = np.random.uniform(low = -1.0, high = 1.0)\n",
    "b = np.random.uniform(low = -1.0, high = 1.0)\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch): \n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    error = np.abs(y_predict-y).mean()\n",
    "    \n",
    "    if error < 0.0005:\n",
    "        break\n",
    "    ## update weights differentially\n",
    "    w1 = w1 - learning_rate * ((y_predict - y)*x1).mean()\n",
    "    w2 = w2 - learning_rate * ((y_predict - y)*x2).mean()\n",
    "    b = b - learning_rate * ((y_predict - y)).mean() ## x3 is always 1\n",
    "    \n",
    "    print(f\"{epoch}, w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:.6f}\")\n",
    "    \n",
    "print(\"-----\"*10)\n",
    "print(f\"{epoch}, w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:.6f}\")\n",
    "\n",
    "### more features available with gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
